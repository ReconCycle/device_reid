{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #! specify gpu here\n",
    "import numpy as np\n",
    "from rich import print\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim, nn\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import clip\n",
    "import exp_utils as exp_utils\n",
    "import albumentations as A\n",
    "from data_loader import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from main import _transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">input resolution <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "input resolution \u001b[1;36m224\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"input resolution\", model.visual.input_resolution)\n",
    "\n",
    "# TODO: what does the CLIP encoder output?\n",
    "\n",
    "seen_classes = [\"hca_0\", \"hca_1\", \"hca_2\", \"hca_2a\", \"hca_3\", \"hca_4\", \"hca_5\", \"hca_6\"]\n",
    "unseen_classes = [\"hca_7\", \"hca_8\", \"hca_9\", \"hca_10\", \"hca_11\", \"hca_11a\", \"hca_12\"]\n",
    "\n",
    "img_path = \"datasets/2023-02-20_hca_backs\"\n",
    "preprocessing_path = \"datasets/2023-02-20_hca_backs_preprocessing_opencv\"\n",
    "\n",
    "# transform_resize = A.augmentations.geometric.resize.LongestMaxSize(max_size=224, always_apply=True)\n",
    "# val_transform = A.Compose([\n",
    "#         # transform_normalise,\n",
    "#         transform_resize,\n",
    "#     ])\n",
    "# train_transform = A.Compose([\n",
    "#         # transform_normalise,\n",
    "#         transform_resize,\n",
    "#     ])\n",
    "#! CHECK IF THESE ACTUALLY DO WHAT preprocess does!!!!\n",
    "\n",
    "train_transform = _transform(224)\n",
    "val_transform = _transform(224)\n",
    "                        \n",
    "dl = DataLoader(img_path,\n",
    "                        preprocessing_path=preprocessing_path,\n",
    "                        batch_size=1,\n",
    "                        seen_classes=seen_classes,\n",
    "                        unseen_classes=unseen_classes,\n",
    "                        train_transform=train_transform,\n",
    "                        val_transform=val_transform,\n",
    "                        cuda=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 224), <f4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/media/ssd2/sruiz/anaconda3/lib/python3.9/site-packages/PIL/Image.py:3070\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3069\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3070\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   3071\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 224), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m img \u001b[39m=\u001b[39m imgs[\u001b[39m0\u001b[39m] \u001b[39m# first in batch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m img_np \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m img_pil \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(img_np)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m display(img_pil)\n",
      "File \u001b[0;32m/media/ssd2/sruiz/anaconda3/lib/python3.9/site-packages/PIL/Image.py:3073\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3071\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3072\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey\n\u001b[0;32m-> 3073\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   3074\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3075\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 224), <f4"
     ]
    }
   ],
   "source": [
    "# only works if we use the non transformed images as input \n",
    "\n",
    "imgs, labels, path, detections = next(iter(dl.dataloaders[\"seen_train\"]))\n",
    "\n",
    "img = imgs[0] # first in batch\n",
    "img_np = img.cpu().numpy()\n",
    "img_pil = Image.fromarray(img_np)\n",
    "\n",
    "display(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_featuress' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (imgs, labels, path, detections) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dl\u001b[39m.\u001b[39mdataloaders[\u001b[39m\"\u001b[39m\u001b[39mseen_train\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# if we need to do preprocessing on raw images:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# for img in imgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# features.append(image_features.cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_image(imgs\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     features\u001b[39m.\u001b[39mappend(image_featuress\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(features)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfrankfurt/home/sruiz/projects/reconcycle/device_reid/plot_clip.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfeatures.shape\u001b[39m\u001b[39m\"\u001b[39m, features\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_featuress' is not defined"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "\n",
    "for i, (imgs, labels, path, detections) in enumerate(dl.dataloaders[\"seen_train\"]):\n",
    "    # if we need to do preprocessing on raw images:\n",
    "    # for img in imgs:\n",
    "        # img_np = img.cpu().numpy()\n",
    "        # img_pil = Image.fromarray(img_np)\n",
    "        # img_pre = preprocess(img_pil).unsqueeze(0).to(device)\n",
    "        # image_features = model.encode_image(img_pre.to(device))\n",
    "        \n",
    "        # features.append(image_features.cpu().detach().numpy())\n",
    "    image_features = model.encode_image(imgs.to(device))\n",
    "    features.append(image_features.cpu().detach().numpy())\n",
    "\n",
    "features = np.array(features).squeeze()\n",
    "\n",
    "print(\"features.shape\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot t-SNE. See plot_clip2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
